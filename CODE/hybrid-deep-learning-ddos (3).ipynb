{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":4852801,"sourceType":"datasetVersion","datasetId":2810740},{"sourceId":4852815,"sourceType":"datasetVersion","datasetId":2810741}],"dockerImageVersionId":31192,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# ============================\n# SEMI-SUPERVISED XGBOOST (BoT + 3% ToN) - CORRECTED FOR KAGGLE GPU\n# ============================\n\n!pip install -q xgboost scipy\n\nimport numpy as np\nimport pandas as pd\nfrom scipy.stats import ks_2samp\nfrom sklearn.preprocessing import QuantileTransformer\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import (\n    accuracy_score, precision_score, recall_score,\n    f1_score, roc_auc_score, classification_report, confusion_matrix\n)\nfrom imblearn.over_sampling import SMOTE\nimport xgboost as xgb\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\nSEED = 42\nnp.random.seed(SEED)\n\n# ============================\n# 1. LOAD DATA\n# ============================\nBOT_PATH = \"/kaggle/input/cicbotiot/CIC-BoT-IoT-V2.parquet\"\nTON_PATH = \"/kaggle/input/cictoniot/CIC-ToN-IoT-V2.parquet\"\n\nFEATURES = [\n    'Flow Duration',\n    'Total Fwd Packets',\n    'Total Backward Packets',\n    'Fwd Packet Length Max',\n    'Bwd Packet Length Max',\n    'Flow Bytes/s',\n    'Flow Packets/s',\n    'Flow IAT Mean'\n]\n\nbot = pd.read_parquet(BOT_PATH, columns=FEATURES + ['Label']).sample(frac=0.05, random_state=SEED)\nton = pd.read_parquet(TON_PATH, columns=FEATURES + ['Label'])\n\nX_bot_df = bot[FEATURES]\ny_bot = bot['Label'].values\n\nX_ton_df = ton[FEATURES]\ny_ton = ton['Label'].values\n\n# ============================\n# 2. DRIFT-AWARE FEATURE SELECTION (TOP-K)\n# ============================\ndrift = []\nfor f in FEATURES:\n    ks, _ = ks_2samp(X_bot_df[f], X_ton_df[f])\n    drift.append((f, ks))\n\ndrift_df = pd.DataFrame(drift, columns=[\"Feature\", \"KS\"]).sort_values(\"KS\")\nTOP_K = 4\nselected_features = drift_df.head(TOP_K)[\"Feature\"].tolist()\nprint(\"Selected features:\", selected_features)\n\nX_bot = X_bot_df[selected_features].values\nX_ton = X_ton_df[selected_features].values\n\n# ============================\n# 3. SEMI-SUPERVISED SPLIT (3% ToN)\n# ============================\nX_ton_train, X_ton_test, y_ton_train, y_ton_test = train_test_split(\n    X_ton, y_ton, train_size=0.03, stratify=y_ton, random_state=SEED\n)\n\nprint(\"Using ToN labeled samples:\", len(X_ton_train))\nprint(\"Evaluating on unseen ToN samples:\", len(X_ton_test))\n\n# ============================\n# 4. SMOTE (BoT ONLY)\n# ============================\nsmote = SMOTE(random_state=SEED)\nX_bot, y_bot = smote.fit_resample(X_bot, y_bot)\n\n# ============================\n# 5. COMBINE DATA\n# ============================\nX_train_all = np.vstack([X_bot, X_ton_train])\ny_train_all = np.hstack([y_bot, y_ton_train])\n\n# ============================\n# 6. ROBUST SCALING\n# ============================\nscaler = QuantileTransformer(\n    n_quantiles=300,\n    output_distribution=\"normal\",\n    random_state=SEED\n)\n\nX_train_all = scaler.fit_transform(X_train_all)\nX_ton_test = scaler.transform(X_ton_test)\n\n# ============================\n# 7. XGBOOST MODEL - CORRECTED FOR KAGGLE\n# ============================\n# First, check if GPU is available\ntry:\n    # Try GPU version first\n    model = xgb.XGBClassifier(\n        n_estimators=700,\n        max_depth=6,\n        learning_rate=0.05,\n        subsample=0.8,\n        colsample_bytree=0.8,\n        tree_method=\"gpu_hist\",  # Try GPU\n        eval_metric=\"logloss\",\n        random_state=SEED,\n        verbosity=0\n    )\n    print(\"Attempting to use GPU acceleration...\")\n    model.fit(X_train_all, y_train_all)\n    print(\"✅ Model trained with GPU acceleration\")\n    \nexcept Exception as e:\n    print(f\"GPU training failed: {e}\")\n    print(\"Falling back to CPU training...\")\n    \n    # Fall back to CPU version\n    model = xgb.XGBClassifier(\n        n_estimators=500,  # Reduced for CPU\n        max_depth=6,\n        learning_rate=0.05,\n        subsample=0.8,\n        colsample_bytree=0.8,\n        tree_method=\"hist\",  # CPU histogram method\n        eval_metric=\"logloss\",\n        random_state=SEED,\n        verbosity=0\n    )\n    model.fit(X_train_all, y_train_all)\n    print(\"✅ Model trained with CPU\")\n\n# ============================\n# 8. EVALUATION (UNSEEN ToN)\n# ============================\nprobs = model.predict_proba(X_ton_test)[:, 1]\npreds = (probs > 0.5).astype(int)\n\nprint(\"\\n=== Semi-Supervised Cross-Dataset (BoT + 3% ToN → 97% ToN) ===\")\nprint(\"Accuracy:\", accuracy_score(y_ton_test, preds))\nprint(\"Precision:\", precision_score(y_ton_test, preds, zero_division=0))\nprint(\"Recall:\", recall_score(y_ton_test, preds, zero_division=0))\nprint(\"F1:\", f1_score(y_ton_test, preds, zero_division=0))\nprint(\"AUC:\", roc_auc_score(y_ton_test, probs))\nprint(\"\\nConfusion Matrix:\")\nprint(confusion_matrix(y_ton_test, preds))\nprint(\"\\nClassification Report:\")\nprint(classification_report(y_ton_test, preds))\n\nprint(\"\\n✅ Semi-supervised training completed\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-20T14:18:34.335696Z","iopub.execute_input":"2025-12-20T14:18:34.336224Z","iopub.status.idle":"2025-12-20T14:19:07.865946Z","shell.execute_reply.started":"2025-12-20T14:18:34.336197Z","shell.execute_reply":"2025-12-20T14:19:07.865060Z"}},"outputs":[{"name":"stdout","text":"Selected features: ['Total Backward Packets', 'Total Fwd Packets', 'Bwd Packet Length Max', 'Fwd Packet Length Max']\nUsing ToN labeled samples: 145424\nEvaluating on unseen ToN samples: 4702075\nAttempting to use GPU acceleration...\n✅ Model trained with GPU acceleration\n\n=== Semi-Supervised Cross-Dataset (BoT + 3% ToN → 97% ToN) ===\nAccuracy: 0.8976985692486827\nPrecision: 0.9774232561715643\nRecall: 0.8372640545299562\nF1: 0.901930986818566\nAUC: 0.9469987769988335\n\nConfusion Matrix:\n[[2009058   51093]\n [ 429936 2211988]]\n\nClassification Report:\n              precision    recall  f1-score   support\n\n           0       0.82      0.98      0.89   2060151\n           1       0.98      0.84      0.90   2641924\n\n    accuracy                           0.90   4702075\n   macro avg       0.90      0.91      0.90   4702075\nweighted avg       0.91      0.90      0.90   4702075\n\n\n✅ Semi-supervised training completed\n","output_type":"stream"}],"execution_count":1}]}